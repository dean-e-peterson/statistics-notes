---
title: "Chapter 6 - Conditional Probability and Independence"
output: html_document
---

*Notes from working through [An Introduction to the Science of Statistics: From Theory to Implementation, Preliminary Edition](https://www.math.arizona.edu/~jwatkins/statbook.pdf) by Joseph C. Watkins*

```{r setup, include=FALSE}
knitr::opts_chunk$set(comment = NA)
```

## 6.1 Restricting the sample space - Conditional Probability

The **conditional probability** of $A$ "given" $B$ is written $P(A|B)$

When all outcomes are equally likely, it can be expressed as # of outcomes as
$$ P(A|B) = \frac{\#(A\cap B)}{\#(B)} $$

Or, more generally, as a probability as

$$ P(A|B) = \frac{\#(A \cap B)/\#(\Omega)}{\#(B)/\#(\Omega)} = \frac{P(A\cap B)}{P(B)} $$

Note that it must be true that $P(B) > 0$.

*Exercise 6.2* Roll two dice. Find $P\{\text{sum is 8}|\text{first die shows 3}\}$ and $P\{\text{sum is 8}|\text{first die shows 1}\}$.  Since all outcomes are equally likely,

$$ P\{\text{sum is 8}|\text{first die shows 3}\} = \frac{P\{\text{sum is 8 and first die shows 3}\}}{P\{\text{first die shows 3}\}} = \frac{1}{6} $$
$$ P\{\text{sum is 8}|\text{first die shows 1}\} = \frac{P\{\text{sum is 8 and first die shows 1}\}}{P\{\text{first die shows 1}\}} = \frac{0}{6} $$

*Exercise 6.3* Roll two four-sided dice.  Find $P(\{\text{sum is at least 5}\})$, $P(\{\text{first die is 2}\})$ , and $P(\{\text{sum is at least 5}|\text{first die is 2}\})$.

$$ P(\{\text{sum is at least 5}\}) = \frac{4}{16} + \frac{3}{16} + \frac{2}{16} + \frac{1}{16} = \frac{10}{16} = \frac58$$

$$ P(\{\text{first die is 2}\}) = \frac14 $$

$$ P(\{\text{sum is at least 5}|\text{first die is 2}\}) = \frac{\frac{2}{16}}{\frac14} = \frac{\frac{1}{8}}{\frac14} = \frac{4}{8} = \frac12$$


## 6.2 The Multiplication Principal

The formula above can be rewritten as **the multiplication principle**

$$ P(A \cap B) = P(A|B)P(B) $$
which, in English, is roughly "The probability of both A and B happening is equal to the probability of A given B times the probability of B happening in the first place."

This can be expanded into a **chain rule**:
$$ P(A \cap B \cap C) = P(A|B \cap C)P(B \cap C) = P(A|B \cap C)P(B|C)P(C) $$

*Exercise 6.5* With $g$ and $b$ being the number of green and blue balls, respectively, show that:

$$
\begin{align*}
{4 \choose 2} \frac{(g)_2(b)_2}{(b+g)_4} &= \frac{{b\choose2}{g\choose2}}{{b+g\choose4}} \\
&= \frac{4!}{2!2!}\cdot\frac{\frac{b}{(b-2)!}\cdot\frac{g}{(g-2)!}}{\frac{b+g}{(b+g-4)!}} \\
&= \frac{\frac{b}{2!(b-2)!}\cdot\frac{g}{2!(g-2)!}}{\frac{b+g}{4!(b+g-4)!}} \\
&= \frac{{b\choose2}{g\choose2}}{{b+g\choose4}}
\end{align*}
$$

## 6.3 The Law of Total Probability

A **partition** of sample space $\Omega$ is a finite collection of pairwise mutually exclusive events $\{C_1, C_2,\dots,C_n\}$ whose union is $\Omega$.

The **law of total probability** 
$$ P(A) = \sum_{i=1}^n P(A|C_i)P(C_i) $$

*Exercise 6.8* [See book]

## 6.4 Bayes Formula

Let $C$ represent the event that the person has a disease.  Let $A$ represent that the person tests positive for the disease.  We know that
$$
\begin{align*}
P(C) &= .001 \\
P(A|C) &= .9 \\
P(A|C^c) &= .01 \\
\end{align*}
$$

**Bayes formula** answers the question, what is $P(C|A)$?  

$$
\begin{align*}
P(C|A) &= \frac{P(A|C)}{P(A)}P(C) \\
&= \frac{P(A|C)P(C)}{P(A|C)P(C) + P(A|C^c)P(C^c)}
\end{align*}
$$

Bayes formula can be generalized to not just $C$ and $C^c$, but to any partition of the sample space where $P(C_i) > 0$ for all $i$.

$$ P(C_j|A) = \frac{P(A|C_j)P(C_j)}{\sum_{i=1}^n P(A|C_i)P(C_i)} $$

## 6.5 Independence

Events $A$ and $B$ are **independent** if the occurence of one does not impact the probability of another.  In other words, independence means

$$
\begin{align*}
P(A) &= P(A|B) \\
P(A)P(B) &= P(A \cap B)
\end{align*}
$$

Events $A_1,\dots,A_n$ are independent if for any choices of them
$$ P(A_i \cap A_j \cap \cdots \cap A_k) = P(A_i)P(A_j)\cdots P(A_k) ? $$

*Exercise 6.13* If $A$ and $B$ are independent events, show that $A^c$ and $B$, $A$ and $B^c$, and $A^c$ and $B^c$ are also independent.

$$
\begin{align*}
P(A) = &P(A|B)P(B) + P(A|B^c)P(B^c) \\
1 - P(A) = &1 - P(A|B)P(B) - P(A|B^c)P(B^c) \\
&???
\end{align*}
$$

*Exercise 6.15* Flip 10 *biased* coins.  Their outcomes are independent with the $i^{th}$ coin being heads with probability $p_i$.  Find 
$$ P\{\text{first heads, third tails, seventh & ninth heads}\} = p_1(1-p_3)p_7p_9$$



