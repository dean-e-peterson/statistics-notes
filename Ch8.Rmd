---
title: "Chapter 8 - The Expected Value"
output: html_document
---

*Notes from working through [An Introduction to the Science of Statistics: From Theory to Implementation, Preliminary Edition](https://www.math.arizona.edu/~jwatkins/statbook.pdf) by Joseph C. Watkins*

```{r setup, include=FALSE}
knitr::opts_chunk$set(comment = NA)
```

## 8.1 Definition and Properties

For data, the **sample mean** of a real-valued function $h$, when $p(x)$ is the proportion of observations that are value $x$.

$$ \overline{h(x)} = \sum_x h(x)p(x)$$

For a probability model, the **expected value** or **expectation** of a random variable $X$, when $P$ is the probability on the sample space $\Omega$

$$ EX= \sum_{j=1}^N X(\omega_j) P\{\omega_j\} $$

The expected value of a real-valued function $g$ of a random vector $X$.

$$ Eg(X) = \sum_{j=1}^Ng(X(\omega_j))P\{\omega_j\} $$

*Exercise 8.2* Fair die and loaded die expected value with $g(x)=x^2$.  Find $EX^2$ for these two examples.

$$ Eg(X) = \frac{1^2}6 + \frac{2^2}6 + \frac{3^2}6 + 
           \frac{4^2}6 + \frac{5^2}6 + \frac{6^2}6 = 
           \frac{91}6 = 15 \frac16 $$

$$ Eg(X) = \frac{1^2}4 + \frac{2^2}4 + \frac{3^2}4 + 
           \frac{4^2}{12} + \frac{5^2}{12} + \frac{6^2}{12} = 
           \frac{3}{12} + \frac{12}{12} + \frac{27}{12} + 
           \frac{16}{12} + \frac{25}{12} + \frac{36}{12} =            
           \frac{119}{12} = 9 \frac{11}{12} $$

"The operation of taking an expectation ... is a **positive linear functional**"  One thing that means is that

$$ E[c_1X_1 + c_2X_2] = c_1EX_1 + c_2EX_2 $$

## 8.2 Discrete Random Variables

"Because sample spaces can be extraordinarily large ... we rarely use the probability space $\Omega$ to compute the expected value."  Instead, partition "the sample space $\Omega$ into the outcomes $\omega$ that result in the same value $x$ for the random variable $X(\omega)$."

"$f_X(x_i) = P\{X=x_i\}$ is the probability mass function for X."

$$ Eg(X) = \sum_{i=1}^n g(x_i)f_X(x_i) = \sum_x g(x)f_X(x)$$

"...is the most frequently used method for computing the expectation of discrete random variables."

[I take this to mean there is no reason to consider each individual outcome for a random variable $X$ if you know the probability mass function which tells you what proportion of outcomes have any given value $x$ of $X$.]

*Exercise 8.5* Draw 5 cards from a standard deck.  Let $X$ be the number of hearts.  Use R to find the mass function for $X$ and use this to find $EX$ and $EX^2$.

```{r}
x <- 0:5
x
dx <- numeric(length(x))
dx[1] <- (39/52)*(38/51)*(37/50)*(36/49)*(35/48)*1
dx[2] <- (13/52)*(39/51)*(38/50)*(37/49)*(36/48)*5
#...
dx[6] <- (13/52)*(12/51)*(11/50)*(10/49)*(9/48)*1
dx
# Gave up solving myself, looked at answer...
hearts <- c(0:5)
f <- choose(13,hearts)*choose(39,5-hearts)/choose(52,5)
f
sum(f)
sum(f*hearts)
sum(f*hearts^2)
```

## 8.3 Bernoulli Trials

**Bernoulli Trials** each have two values, success (1) or failure (0).  Each trial has the same success probability $p$, and the outcomes of each trial is independent of the others.

If $S_n = X_1 + X_2 + \cdots + X_n$ is the total number of successes in $n$ Bernoulli trials, then the expected number of successes is.

$$ ES_n = np $$

The probability of $n$ Bernoulli trials having $x$ successes is $p^x(1-p)^{n-x}$.  There are ${n \choose x}$ different orders of $n$ Bernoulli trials having $x$ successes.  Combining those two facts gives a probability mass function of

$$ f_{S_n} = P\{S_n = x\} = {n \choose x} p^x(1-p^{n-x}) $$
$S_n$ is called a **binomial random variable**.

## 8.4 Continuous Random Variables

$$ Eg(X) = \int_{-\infty}^\infty g(x)f_X(x)dx $$
To solve this, we use integration by parts, where

$$ \int u(x) v(x) dx = u(x) \int v(x) dx - \int u'(x) (\int v dx) dx$$
...or something like that?  ($u'(x) is the derivative of u(x)).

While the area under the density function curve is 1, if I understand correctly, the expected value is the area under the survival function $\bar F_X (x) = 1 - F_X(x)$ (or, alternately, the "expected value is the area between the cumulative distribution function and the line $y=1$).  [I dont know if this is always the case.]

"The most important density function we shall encounter is for $Z$, the **standard normal random variable**."

$$ \phi(z) = \frac{1}{\sqrt{2\pi}} \exp(-\frac{z^2}{2}), z \in \mathbb{R}$$

The cumlative distribution function 

$$ \Phi(z) = P\{Z\leq z\}$$

is calculated using numerical approximation instead of a formula.  R uses `pnorm(z)` for this.

The **standardized version** of X is

$$ Z = \frac{X-\mu}{\sigma}$$

## 8.6 Names for $Eg(X)$

[See book if you want details, but here are some highlights...]

| Moment? | Standard normal random variable | Something like it | is this to distribution |
| --- | --- | --- | --- |
| First | $$EZ=0$$ | $$\mu = EX$$ | (distributional) mean |
| Second | $$EZ^2=1$$ | $$\sigma^2 = E(X - \mu)^2$$| (distributional) variance |
| Third | $$EZ^3=0$$ | $$\gamma_1 = E \left[\left(\frac{X-\mu}{\sigma}\right)^3\right]$$| skewness |
| Fourth | $$EZ^4=3$$ | $$E \left[\left(\frac{X-\mu}{\sigma}\right)^4\right] - 3$$| kurtosis |

*Exercise 8.21* Quadratic identity for variance
$$ Var(a+bX) = b^2Var(x) $$

The **standard inner product** of two vectors is 
$$ \langle \mathbf x, \mathbf y \rangle = \sum_{i=1}^d x_iy_i $$

## 8.7 Independence

Expectation for multiple random variables is based on the **joint mass function** $f_{X_1,X_2}(x_1,x_2)$

"The Expectation of the product of two **independent** random variables equals the product of the expectation."

$$ E[g_1(X_1)g_2(X_2)] = E[g_1(X_1)]\cdot E[g_2(X_2)] $$

## 8.8 Covariance and Correlation

The **covariance** is 

$$ Cov(X_1, X_2) = E[(X_1 - \mu_1)(X_2 - \mu_2)] \,  ?= E[X_1X_2] - \mu_1\mu_2$$

$$ Var(X_1 + X_2) = Var(X_1) + 2 Cov(X_1, X_2) + Var(X_2)$$

The **correlation** is

$$ \rho (X_1, X_2) = \frac{Cov(X_1, X_2)}{\sqrt{Var(X_1)}\sqrt{Var(X_2)}} $$

*Exercise 8.24* Find the variance of a binomial random variable based on $n$ trials with success parameter $p$.  ###

$$ \text{Var}(b) = E[(b - E(b))^2] \,\,? $$

## 8.9 Quantile Plots and Probability

A **Q-Q plot** for the normal distribution is also called a **normal probability plot**.  If the data follow a normal distribution, the plot should follow a straight line with a slope $s_x$ and y-intercept $\bar x$

```{r}
length(morley[,3])
mean(morley[,3])
sd(morley[,3])
par(mfrow=c(1,2))
hist(morley[,3])
qqnorm(morley[,3])
```

