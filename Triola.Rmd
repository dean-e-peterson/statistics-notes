---
title: "Triola"
output: html_document
---

Notes from working through *Essentials of Statistics, 6th Ed.*, by Mario F. Triola

```{r setup, include=FALSE}
knitr::opts_chunk$set(comment = NA)
```

## Chapter 4 - Probability

### Chapter 4-2 - Addition Rule and Multiplication Rule

1. $P(A)$ is the probability that a randomly selected adult has blue eyes
   $P(\bar A)$ is the probability that a randomly selected adult does not have blue eyes.
   
3. They are dependent, but can be considered independent for the purposes of calculations because the sample is (well) under 5% of the population.

5. 74%

7. $P(\bar N) = .670$, and represents surveyed adultswho do travel on commercial flights.

9. 67.6%
```{r}
1 - ((329 + 33) / (329 + 264 + 249 + 145 + 33 + 54 + 31 + 13))
```

11. 906. No, they are not disjoint.
```{r}
qsr <- matrix(c(329, 33, 264, 54, 249, 31, 145, 13), nrow = 2)
rownames(qsr) <- c("Order Accurate", "Order Inaccurate")
colnames(qsr) <- c("McDonald's", "Burger King", "Wendy's", "Taco Bell")
qsr
(sum(qsr[1,]) + sum(qsr[,1]) - qsr[1,1]) / sum(qsr)
```

13. a) Yes, the events are independent, and the probability is .01997
    b) No, the events are dependent, and the probability is 0.01986 

```{r}
(sum(qsr[, "Taco Bell"]) / sum(qsr))^2
sum(qsr[, "Taco Bell"]) / sum(qsr) * (sum(qsr[, "Taco Bell"]) - 1) / (sum(qsr) - 1)
```

15. a) Yes, the events are independent, and the probability is .7794
    b) No, the events are dependent, and the probability is .7793
```{r}
(sum(qsr["Order Accurate",]) / sum(qsr))^2
sum(qsr["Order Accurate",]) / sum(qsr) * (sum(qsr["Order Accurate",]) - 1) / (sum(qsr) - 1)
```

19. .0156

```{r}
sum(qsr[,"Wendy's"]) / sum(qsr) * (sum(qsr[,"Wendy's"])-1) / (sum(qsr) - 1) * (sum(qsr[,"Wendy's"])-2) / (sum(qsr) - 2) 

(factorial(sum(qsr[,"Wendy's"])) / factorial(sum(qsr[,"Wendy's"]) - 3)) / (factorial(sum(qsr)) / factorial(sum(qsr) - 3))

```

### 4.3 Complements, Conditional Probability, and Bayes' Theorem

5. Probability of 3 boys = $\frac12^3 = \frac18$.  Probability of the complement, at least one girl, = $\frac78$, or .875

7. .982

```{r}
1 - .512^6
```

9. .344

```{r}
1 - .9^4
```

11. .965

```{r}
1 - .8^15
```

13. a. .628
    b. .372
    c. That if you give someone the right denomination to purchase something, they are more likely to do so compared to breaking a higher denomination.
```{r}
27 / (27 + 16)
16 / (27 + 16)
```

17. .00173

```{r}
2/1155
335/337
```

19. .994

21. a. .9991
    b. .999973

```{r}
1 - .03^2
1 - .03^3
```

23. .490

```{r}
1 - (1-.126)^5
```

25. Birthday, 25 people, .569

```{r}
1 - prod((365 - 0:24) / 365)
```

### 4.4 Counting

3. Permutations rule with repeated values?, because order matters
   NOPE: Multiplication rule.
   
5. 1/10000, or .0001

7. 684, nope 1/171

```{r}
factorial(19) / (factorial(17) * factorial(2))
```

9. 1/40320, or .0000248

```{r}
1 / factorial(8)
```

11. 1/254251200

```{r}
factorial(50) / factorial(50 - 5)
```

13. 1/100,000,000.  No

```{r}
100^4
```

15. 1/1820

```{r}
choose(16,4)
factorial(16) / (factorial(16-4) * factorial(4))
```

17. 1/292,201,338

```{r}
choose(69, 5) * 26
```

19. 1/100,000

21. 800, 6,400,000,000

```{r}
800 * 800 * 10000
```

23. a. 5040
    b. 210
    c. 1/5040, nope 1/210

```{r}
factorial(10)/factorial(10-4)
factorial(10)/(factorial(10-4)*factorial(4))
```

25. 40,320, 1/40,320

```{r}
factorial(8)
```

27. 20,922,790,000,000, no 653,837,184,000, forgot repeat letters.


```{r}
factorial(16) / (2*2*2*2*2)
```

31. 62

```{r}
2^5 + 2^4 + 2^3 + 2^2 + 2^1
```

33. 4.7%.  Nope, 4.8% 

```{r}
(4/52) * (16/51) * 2
```

35. 

## 5. Discrete Probability Distributions

### 5.1 Probability Distributions

1. The number of girls born, integers 0 through 4 inclusive, and yes, its values are numerical.

3. Yes, not counting rounding errors, they sum to 1.  It is a probability distribution because additionally all values are between 0 and 1 inclusive.

5. a. Continuous random variable
   b. Not a random variable
   c. Discrete random variable
   d. Continuous random variable
   e. Discrete random variable?  (Assuming you consider shoe sizes numerical, not categorical)

7. Yes, it's a probability distribution.  Mean = 2.5, sd = 1.246, nope, 1.1

9. No, it's not a probability distribution because the probabilities don't sum to 1.

11. Yes, it's a probability distribution.  Mean = 1.648 (1.6). sd=.744, nope .862 (.9)

13. No, because there is no numerical random variable.

15. Mean = 4.0 (3.996), Sd=1.4 (1.418)

```{r}
births <- 0:8
prob <- c(.004, .031, .109, .219, .273, .219, .109, .031, .004)
births
sum(prob)
girls <- data.frame(births, prob)
mu = sum(girls$births * girls$prob)
sd = (sum(girls$births^2 * girls$prob) - mu^2) ^ .5
mu
sd
```

17. No

19. a. .109
    b. .144
    c. part b
    d. No, because the probability of that many or more girls is .144, which is greater than .05.
    
```{r}
sum(girls$prob[girls$births >= 6])
```

21. Mean = 1.5 (1.48), sd = 1.0 (1.019)

```{r}
sleep <- data.frame(x=0:5, p=c(.172, .363, .306, .129, .027, .002))
mean = sum(sleep$x * sleep$p)
mean
sd = sum((sleep$x - mean)^2 * sleep$p)^.5
sd
mean + 2*sd
sum(sleep$p[sleep$x <= 1])
```

23. No

25. a. .363
    b. .535
    c. part b
    d. No, because the probability of a result that or more extreme is (much) greater than .05.

27. a. 1000
    b. 1/1000
    c. $499
    d. .5
    e. Neither

```{r}
-1 * .999 + 499 * .001
```
    
29. a. -$161 vs $99839
    b. -$21
    c. Yes, because on average the consumer loses money.

```{r}
100000 - 161
-161 * .9986 + (100000-161) * (1-.9986)
```

### 5.2 Binomial Probability Distributions

1. It does not account for the possibility that the two customers that want drone deliveries could appear in any order withing the five people being considered.  Therefore, it underestimates the probability.

3. No, they are dependent.  They can be treated as independent, though, because 30 out of 1009 is less than 5% for a sample size.  No, the probability cannot be found directly with the binomial probability formula, because that formula gives probablities for exactly x successes, not "at least" x successes.  NOPE, yes, you can use the formula, but must use it multiple times, basically for the reasons I stated.

5. No, because the observed variable is not in the form of a simple success or failure.

7. Yes, it's binomial.

9. No, because without replacement, the events are not independent, and since the sample is larger than 5%, the events cannot be treated as independent.

11. Yes, it's binomial.

13. a. $\frac45 \cdot \frac45 \cdot \frac15 = \frac{16}{125}$
    b. WWC, WCW, CWW, each with probability $\frac{16}{125}$
    c. $\frac{48}{125}$
    
```{r}
16/125
48/125
```

$$P(x) = {n \choose x}p^x(1-p)^{n-x} = \frac{n!}{(n-x)!x!}p^x(1-p)^{n-x}$$

15. .000002048 Nope, .0000819

```{r}
n <- 8
x <- 7
p <- .2
(factorial(n)/(factorial(n-x)*factorial(x))) * p^x * (1-p)^(n-x)
```

17. .797
```{r}
pbinom(2, size=8, prob=.2)
p <- .2
n <- 8
x <- 0
a0 <- (factorial(n)/(factorial(n-x)*factorial(x))) * p^x * (1-p)^(n-x)
x <- 1
a1 <- (factorial(n)/(factorial(n-x)*factorial(x))) * p^x * (1-p)^(n-x)
x <- 2
a2 <- (factorial(n)/(factorial(n-x)*factorial(x))) * p^x * (1-p)^(n-x)
a0 + a1 + a2
```

19. .168
```{r}
a0
```

21. .147

```{r}
n <- 8
x <- 6
p <- .54
factorial(n)/(factorial(n-x)*factorial(x))
choose(n, x) * p^x * (1-p)^(n-x)
```

23. .089

```{r}
1 - pbinom(7, size=10, prob=.54)
```

25. .00000451

```{r}
x <- 90
n <- 7
p <- .27
choose(x, n) * p^n * (1-p)^(x-n)
pbinom(7, size=90, prob=.27)
```

27. a. .00154
    b. .000064
    c. .0016
    d. Yes, its probability is less than .05

```{r}
factorial(6) / (factorial(1) * factorial(5)) * .2^5 * .8^1
factorial(6) / (factorial(0) * factorial(6)) * .2^6
.001536 + .000064
1  - pbinom(4, size=6, prob=.2)
```

29. a. mean=18, sd=3
    b. 12 and 24
    c. No, it is significantly low, and suggests the treatment had the opposite effect.  NOPE, yes.

```{r}
xsort.var <- 36 * .5 * .5
xsort.var^.5
```

31. a. mean = 7.5, sd = 1.9 NOPE, sd is 1.4
    b. 3.8 and 11.3 NOPE, 4.8 and 10.2
    c. No, 9 is not significantly high because it is within 2 sd of the mean.

```{r}
n <- 10
p <- .75
n*p
(n*p*(1-p))^.5
n*p - 2*(n*p*(1-p))^.5
n*p + 2*(n*p*(1-p))^.5
```

33. .304. No.

```{r}
p <- .01
n <- 36
1 - (1-p)^36
1 - pbinom(0, size=n, prob=p)
1 - (factorial(36)/(factorial(36) * factorial(0)) * .01^0 * .99^36)


```

35. .338, many will be rejected.  NOPE, .662

```{r}
n <- 40
p <- .03
x = 1
1 - pbinom(x, size=n, prob=p)
```

37. a. 8.7 and 23.3.  No
    b. .0736
    c. .242
    d. part c, No, because the probability is greater than .05
    e. No reason to doubt.

```{r}
p <- .16
n <- 100
mm.mean <- n * p
mm.sd <- (n * p * (1-p))^.5
mm.mean + 2*mm.sd
mm.mean - 2*mm.sd
dbinom(19, size=n, prob=p)
1 - pbinom(18, size=n, prob=p)
```

39. a. Yes
    b. .0000369
    c. .0000987, NOPE .000136
    d. part c, yes
    e. They appear to be inaccurate.

```{r}
n <- 611
p <- .43
pres.mean <- n * p
pres.mean
pres.sd <- (n * p * (1-p))^.5
pres.mean - 2*pres.sd
pres.mean + 2*pres.sd
x <- 308
choose(n, x) * p^x * (1-p)^(n-x)
1 - pbinom(x - 1, size=n, prob=.43)
```

41. .0468

```{r}
(1-.06)^4 * .06
```

43.

```{r}

```

### 5.3 Poisson Probability Distributions

1. $\mu=\frac{535}{576} = .929$,
   $x = 2$,
   $e = 2.718$
   
   mu is the average number of buzz bombs per region
   x is the number of buzz bombs hitting a region that we are looking for the probability of.
   e is Euler's constant, deal with it.

```{r}
mu <- 535/576
x <- 2
e <- exp(1)
mu
x
e
```

3. x is a discrete random variable whic can take on integer values from 0 to infinity.  x=2.3 is not possible.

5. a. .158
   b. 8.682
   c. Compares well.  Yes.

```{r}
6.1^5 * exp(-6.1) / factorial(5)
55 * (6.1^5 * exp(-6.1) / factorial(5))
```

7. a. .140
   b. 7.7
   c. Compares well. Yes.

```{r}
6.1^7 * exp(-6.1) / factorial(7)
55 * (6.1^7 * exp(-6.1) / factorial(7))
```

9. .0643

```{r}
mu <- 4221 / 365
mu
x <- 15
mu^x * exp(-mu) / factorial(x)
```

11. I don't think half-life is Poisson, but...
    a. 62.2
    b. .0155

```{r}
mu <- 22713/365
mu
mu^50 * exp(-mu) / factorial(50)
```

13. a. .170
    b. 98.1
    c. Pretty close

```{r}
mu <- 535 / 576
mu
mu^2 * exp(-mu) / factorial(2)
576 * mu^2 * exp(-mu) / factorial(2)
```

15. .9999877

```{r}
(mu <- 33561 / 2969)
1 - (mu^0 * exp(-mu) / factorial(0))
```

17. .0000180  NOPE .0000178, rounding?

```{r}
n <- 5200
(p <- 1/292201338)
(mu <- n * p)
x <- 0

1 - (mu^x * exp(-mu) / factorial(x))
```

## 6. Normal Probability Distributions

### 6.1 The Standard Normal Distribution

1. That's not what normal distribution means.  Those numbers are more likely in a uniform distribution.

3. mean of zero, standard deviation of 1

5. .4

7. .2

9. .6700

11. .6993

13. 1.23

15. -1.45

17. .1093

19. .8997

21. .4013

23. .9772

25. .0214

27. .0174

29. .9545

31. .8413

33. .9999973

35. .5000

37. 2.33

```{r}
qnorm(.99)
```

39. -2.05 and 2.05

```{r}
qnorm(.02)
qnorm(1 - .02)
```

41. .8159 NOPE 1.28

```{r}
qnorm(1-.1)
qnorm(1 - .04)
```

43. .8315 NOPE 1.75

45. 68.3%

```{r}
pnorm(1) - pnorm(-1)
```

47. 99.73%
```{r}
pnorm(3) - pnorm(-3)
```

49. a. 2.275%
    b. 2.275%
    c. 95.45%
    

```{r}
1 - pnorm(2)
pnorm(-2)
pnorm(2) - pnorm(-2)
```

### 6.2 Real Applications of Normal Distributions

1. a. mean 0, sd 1
   b. unitless
   
3. A standard normal distribution has a mean of 0 and a standard deviation of 1.  Any other values for mean and standard deviation make a non-standard normal distribution.

5. .8849

7. .9053

9. 135.998

11. 69.26

13. .0115

15. .6612

17. 24.91

19. 20.9 to 26.1 inches.  No

21. a. 72.11%, too short.
    b. 58.25" to 69.15"
    
23. a. .92%, which suggests that most Mickey Mouses are women.
    b. 64.0 - 68.6 inches.

```{r}
m.mean <- 68.6
m.sd <- 2.8
pnorm((62 - m.mean) / m.sd) - pnorm((56 - m.mean) / m.sd)
```
```{r}
qnorm(1 - .5) * m.sd + m.mean
qnorm(.05) * m.sd + m.mean

```

25. .201, no

```{r}
1 - pnorm((230 - 184) / 55)

```


35. a. mean=75, sd=12
    b. No?
    c. 66 to 75
    d. ?

### 6.3 Sampling Distributions and Estimators 

1. a. They will be .512, they will target the population mean
   b. It will be a normal distribution (APPROXIMATELY)

3. Unbiased estimators: sample mean, sample proportion, sample variance.

5. No, because it is an estimator of a more specific population, that of boys born in China.

7. a. Population variance $\sigma^2=4.667$
   c. Mean of sampling distribution of sample variances $s^2=4.667$
   d. Yes, the sample variance is an unbiased estimator of the population variance, because the mean of the sample variance converges on the population variance as you construct the entire sampling distribution.

```{r}
kids <- c(4,5,9)
kids.mu <- sum(kids) / length(kids)
kids.popvar <- sum((kids - kids.mu)^2) / length(kids)
kids.popvar

samp <- data.frame(a = c(4,4,4,5,5,5,9,9,9), 
                   b = c(4,5,9,4,5,9,4,5,9))
samp$mean <- (samp$a + samp$b) / 2
samp$var <- ((samp$a - samp$mean)^2 + (samp$b - samp$mean)^2) / 1
samp
sum(samp$var) / length(samp$var)
```

9. a. 5
   c. 6
   d. No, it's biased

```{r}
median(kids)
# Only two values in each sample, so
samp$median <- samp$mean
samp$median
sum(samp$median) / length(samp$median)
```

### 6.4 The Central Limit Theorem

1. If they know that the original population of grade-point averages has a normal distribution, or if the sample size that she took is greater than 30.

3. $\mu_\bar x$ is the mean of the sampling distribution means.
   $\sigma_\bar x$ is the standard deviation of the sampling distribution means.

```{r}
pnorm(80, mean=74, sd=12.5)
pnorm(80, mean=74, sd=12.5/(16^.5))
```

5. a. .6844
   b. .9726
   c. Because we assumed that the original population of females that the sample was drawn from had pulse rates that are normally distributed.
   
```{r}
pnorm(76, mean=74.0, sd=12.5) - pnorm(72, mean=74.0, sd=12.5)
pnorm(76, mean=74.0, sd=12.5/2) - pnorm(72, mean=74.0, sd=12.5/2)
```

7. a. 0.1271
   b. 0.2510
   c. ditto 5c.
   
```{r}
1 - pnorm(185, mean=189, sd=39/(27^.5))
```
   
9. .2970, NOPE, .7030, Not if 27 adult males really fit in that elevator.

```{r}
qnorm((1-.02), mean=100, sd=15)
1 - pnorm(131, mean=100, sd=15/2)
```

11. a. 130.8
    b. .00001788
    c. No.
    
```{r}
3500/25
pnorm(140, mean=189, sd=39/5)
1 - pnorm(140, mean=189, sd=39/5)
1 - pnorm(175, mean=189, sd=39/(20^.5))
```

13. a. 140 pounds
    b. 1 (.9999999998..., etc)
    c. .9458
    d. No.
    
```{r}
1 - pnorm(17, mean=14.4, sd=1)
1 - pnorm(17, mean=14.4, sd=1/(122^.5))
```

15. a. .004661
    b. 0
    c. part a

```{r}
pnorm(211, mean=171, sd=46) - pnorm(140, mean=171, sd=46)
pnorm(211, mean=171, sd=46/5) - pnorm(140, mean=171, sd=46/5)
```

17. a. .5575
    b. .9996
    c. part a.  You only eject 1 person at a time.

```{r}
pnorm(72, mean=68.6, sd=2.8)
pnorm(72, mean=68.6, sd=2.8/10)
```
    
19. a. .8877
    b. 1.
    c. part a
    d. Because they are on average shorter than men?

```{r}
x <- (16/(50^.5)) * (275-50)^.5/(275-1)^.5
x
pnorm(105, mean=95.5, sd=x) - pnorm(95, mean=95.5, sd=x)
```

    
21. a. Yes, because the sample size is greater than 5% of the population.  The correct $\sigma_\bar x$ is 2.0505
    b. 0.5963
    
### 6.5 Assessing Normality

1. The histogram should have a symmetric bell shape.  The normal quantile plot would have the points close to a straight line, with no additional pattern.

3. The original population would have to be normally distributed.  You could use a histogram, look for outliers, and use a normal quantile plot.

5. Yes, normal

7. Non-normal.  There is a pattern in addition to a straight line.

9. Normal?
```{r}
cookies <- read.table("data\\Triola\\28 - Chocolate Chip Cookies.txt", header=T, sep="\t")
par(mfrow=c(1,2))
hist(cookies$CHIPS.AHOY.RED.FAT)
qqnorm(cookies$CHIPS.AHOY.RED.FAT)

```

11. Non-normal

```{r}
garbage <- read.table("data/Triola/31 - Garbage Weight.txt", header=T, sep="\t")
garbage$YARD
par(mfrow=c(1,2))
hist(garbage$YARD)
qqnorm(garbage$YARD, datax = T)
```

13. Normal

```{r}
par(mfcol=c(1,2))
hist(cookies$CHIPS.AHOY.RED.FAT)
qqnorm(cookies$CHIPS.AHOY.RED.FAT, datax=TRUE)
```

15. Non-normal

17. 40.7 44.3 34.2 32.5 38.5  Sample to make a graph manually from

19. 1027, 1029, 1034, 1070, 1079, 1079, 963, 1439

21. a. Adding leaves heights normally distributed.
    b. Multiplying leaves heights normally distributed.
    c. Logarithms of heights look slightly different, but still normally distributed.  NOPE, supposed to say NO.
    

```{r}
body <- read.table("data/Triola/01 - Body Data.txt", header=T, sep="\t")
height <- body[body[,2] == 1,]$HEIGHT
par(mfrow=c(1,2))
hist(height)
qqnorm(height, datax=T)

hist(height + 5)
qqnorm(height + 5, datax=T)

hist(height/2.5)
qqnorm(height/2.5, datax=T)

hist(log(height))
qqnorm(log(height), datax=T)

hist(log10(height))
qqnorm(log10(height), datax=T)
```

### 6.6 Normal as Approximation to Binomial

1. a. <= 502.5
   b. 501.5 <= p <= 502.5
   c. >= 502.5

3. p=.2, q=.8, $\mu=20$, $\sigma=4$,
   $\mu$ measures the mean, the expected, number of correct answers.
   $\sigma$ measures the variation in the number of correct answers.

```{r}
(.2 * .8 * 100)^.5
```

5. np and nq are big enough.
   .1102

```{r}
pnorm(7.5, mean=20*.512, sd=(20*.512*(1-.512))^.5)
pbinom(7, size=20, prob=.512)
```

7. np is < 5, so a normal approximation should not be used.

```{r}
20 * .2
```

9. .2028

```{r}
pnorm(19.5, mean=100*.23, sd=(100*.23*(1-.23))^.5)
pbinom(19, size=100, prob=.23)
```

11. .05487, because it is the probability for that exact number of cars, not that number or higher.

```{r}
mu <- 100 * .10
sigma <- (100 * .10 * .9)^.5
pnorm(14.5, mean=mu, sd=sigma) - pnorm(13.5, mean=mu, sd=sigma)
dbinom(14, size=100, prob=.10)
```

13. a. .02117
    b. .2012, no, it's not less than 5%

```{r}
mu <- 879 * .25
sigma <- sqrt(879 * .25 * .75)
pnorm(231.5, mean=mu, sd=sigma) - pnorm(230.5, mean=mu, sd=sigma)
dbinom(231, size=879, prob=.25)
1 - pnorm(230.5, mean=mu, sd=sigma)
1 - pbinom(230, size=879, prob=.25)
```

15. a. .01138
    b. Yes, it is significantly low.

```{r}
n <- 250
p <- .51
pnorm(109.5, mean=n*p, sd=sqrt(n*p*(1-p)))
pbinom(109, size=n, prob=p)
```


21. ?, .170, .172

```{r}
n <- 11
p <- .512
mu <- n * p
sigma <- sqrt(n * p * (1-p))
pnorm(7.5, mean=mu, sd=sigma) - pnorm(6.5, mean=mu, sd=sigma)
dbinom(7, size=n, prob=p)
```

## 7. Estimating Parameters and Determining Sample Sizes

### 7.1 Estimating a Population Proportion

1. Confidence level

3. $\hat p$ is sample proportion whose favorite pie was chocolate
   $\hat q$ is sample proportion whose favorite pie was NOT chocolate
   $n$ is the sample size
   $E$ is the margin of error
   $p$ is the true population proportion whose favorite is chocolate.
   $\alpha$ is .95. NOPE .05

5. 1.645
7. 2.807
9. $.1302 \pm .0868$
11. $.07995 - .06305 < p < .07995 + .06305$ Nope $0.0169 < p < 0.143$
13. a. .0912
    b. .0297
    c. .0912 += .0297
    d. We are confident that 95% of the time, the confidence interval contains the true proportion.

```{r}
phat <- 33/362
phat
qnorm(.975) * sqrt(phat * (1-phat)/362)

```

15. a. .143
    b. .00815
    c. .135 < p < .152
    d. We are confident that 90% of the time, the true proportion p will be between .135 and .152

```{r}
phat <- 717 / 5000
phat
e <- qnorm(.95) * sqrt(phat*(1-phat)/5000)
e
phat + e
phat - e
```

17. .462 < p < .529.  No.

```{r}
n <- 860
phat <- 426 / n
phat
e <- qnorm(.975) * sqrt(phat*(1-phat)/n)
e
phat - e
phat + e
```

19. 11.6% < p < 22.4%.  That it is possible that either restaurant could have more inaccurate orders based on 99% confidence levels.

```{r}
n <- 264 + 54
phat <- 54 / n
phat
e <- qnorm(.995) * sqrt(phat * (1-phat) / n)
e
(phat - e) * 100
(phat + e) * 100
```

21. a. .5
    b. .439
    c. .363 < p < .516
    d. No evidence it is better than chance.

```{r}
n <- 280
phat <- 123/n
phat
e <- qnorm(.995) * sqrt(phat * (1-phat) / n)
phat - e
phat + e
```


31. a. 1844
    b. 664
    c. Doesn't affect the required sizes at all.

```{r}
qnorm(.995)^2 * .25 / .03^2
qnorm(.995)^2 * .1 * .9 / .03^2
```

33. a. 385
    b. 369
    c. No

```{r}
qnorm(.975)^2 * .25 / (.05)^2
qnorm(.975)^2 * .4 * .6 / (.05)^2
```

35. a. 1537
    b. 1449

```{r}
qnorm(.975)^2 * .25 / .025^2
qnorm(.975)^2 * .38 * (1-.38) / .025^2
```

###7.2 Estimating a Population Mean

1. a. 13.046 < p < 22.15 ROUND BETTER
   b. 17.598 and ME 4.552
   c. Because the sample size is > 30
   
```{r}
22.15 - 17.598
17.598 - 13.046
```
   
3. If we followed this same procedure multiple times, we would get confidence intervals that contained the true population mean 95% of the time.

5. Neither distribution applies.  (READ FROM BOOK)
7. Neither or normal z distribution, 2.576

```{r}
qnorm(.995) * 3342/ sqrt(61)
qnorm(.995)
```

9. (29.4, 31.4),  No 

```{r}
me <- qnorm(.975) * 7.1 / sqrt(205) # WAIT, 
me
30.4 - me
30.4 + me
```

11. $98.08 < \mu < 98.32$  Suggests it's false.

```{r}
me <- qt(.975, df=105) * .62 / sqrt(106)
98.2 - me
98.2 + me
```

13. 71.4 < mu < 126.4.  No significant evidence to reject the null hypothesis that zopiclone is ineffective. 

```{r}
xbar <- 98.9
sem <- 42.3 / sqrt(16)
xbar + qt(.01, df=15) * sem
xbar + qt(.99, df=15) * sem
```

29. 94 professors.  Practical.

```{r}
(qnorm(.995) * 15 / 4)^2
```

31. 38,415, impractical.

```{r}
(qnorm(.975) * 1 / .01)^2
```


### 7.3 Estimating a Population Standard Deviation or Variance

1. $95.0 < \sigma < 182.5$
   We are 95% confident that the value of the population standard deviation is between 95.0 and 182.5.

3. No normally distributed.
   Cannot be treated as normally distributed.
   Using bootstrapping, but not through parametric means.
   
5. $.19 < \sigma < .33$

```{r}
chil <- qchisq(.025, df=24)
chir <- qchisq(.975, df=24)
chil
chir
s <- .24
l <- sqrt(24 * .24^2 / chir)
r <- sqrt(24 * .24^2 / chil)
l
r
```

7. $df=146, \chi^2_L=105.7, \chi^2_R=193.8, 56.8 < \sigma < 76.8$

```{r}
df <- 147 - 1
chil <- qchisq(.005, df=df)
chir <- qchisq(.995, df=df)
chil
chir
sqrt(df * 65.4^2 / chir)
sqrt(df * 65.4^2 / chil)

```

9. $.55 < \sigma < .72$

```{r}
df <- 106 - 1
chil <- qchisq(.025, df=df)
chir <- qchisq(.975, df=df)
l <- sqrt(df * .62^2 / chir)
r <- sqrt(df * .62^2 / chil)
l
r
```

11. $29.6 < \sigma < 71.6$
    No.

```{r}
df <- 16 - 1
s <- 42.3
sqrt(df * s^2 / qchisq(.99, df=df))
sqrt(df * s^2 / qchisq(.01, df=df))

```

13. $1.6 < \sigma < 3.8$

```{r}
dates <- c(7,8,2,10,6,5,7,8,8,9,5,9)
sd(dates)
df <- length(dates) - 1
df
sqrt(df * sd(dates)^2 / qchisq(.975, df=df))
sqrt(df * sd(dates)^2 / qchisq(.025, df=df))

```


### 7.4 Bootstrapping

1. Bootstrapping requires sampling with replacement to allow samples of the same size as your original sample to come up with different combinations.  A sample of the same size as the original, done without replacement, is by definition exactly the same sample.

3. b, d, and e

5. 0 < p < .5

```{r}
x <- c(0,.5,.5,0,0,.25,0,0,.25,.5)
sort(x)
```


## 8. Hypothesis Testing

### 8.1 Basics of Hypothesis Testing

1. The rejection of the Bayer aspirin claim is the more serious matter because aspirin has more direct effects, including the prevention of heart attacks.  I'm guessing no?  SPECIFICALLY, "It would be wise to use a smaller significance level for testing the claim about the aspirin."

3. a. $H_0: \mu = 174.1$
   b. $H_1: \mu \neq 174.1$
   c. reject $H_0$, or fail to reject $H_0$
   d. No.

5. a. $p > .5$
   b. $H_0: p = .5$
      $H_1: p > .5$

7. a. $\mu = 69$
   b. $H_0: \mu = 69$
      $H_1: \mu \neq 69$
      
9. Subjectively, we would reject the null hypothesis, and find there IS sufficient evidence to conclude that most (more than half) of people want to erase all of their online data.

11. Subjectively, we would fail to reject the null hypothesis, and find there IS NOT sufficient evidence to conclude that the actual adult male heartrate mean is different from 69 bpm.

13. $$z = \frac{\hat p - p}{\sqrt{\frac{pq}{n}}}$$, 4.279

```{r}
(.59 - .5)/sqrt(.5*.5/565)
```

15. t= .66

```{r}
(69.6 - 69)/(11.3/sqrt(153))
```

17. a. right-tailed
    b. P .159
    c. fail to reject null hypothesis

```{r}
1 - pnorm(1)
```

19. a. two-tailed
    b. P .0444
    c. reject null hypothesis

```{r}
(1 - pnorm(2.01)) * 2
```

21. a. 1.645
    b. fail to reject null hypothesis

```{r}
qnorm(.95)
```

23. a. 1.960  (SHOULD HAVE said +/-)
    b. reject null hypothesis

```{r}
qnorm(.975)
```

25. a. Reject the null hypothesis
    b. Evidence supports the claim that more than half of adults would erace all of their personal information online if they could.
    OH, that's not the claim, um. OK.
    
27. a. Reject the null hypothesis
    b. There is sufficent evidence to reject the claim that adult males have a mean pulse rate of 72bpm.

29. Type I error: Reject that p=.1 when actually it is true that p=.1
    Type II error: Fail to reject that p=.1 when p does not = .1.
    
31. Type I error: Reject that p=.87 when actually p=.87
    Type II error: Fail to reject that p=.87 when p > .87

33. .96 is the probability of rejecting a false null hypothesis if .18 is the alternate value of p.  It is the probability of rejecting the claim that 8% of Chantix users experience adominal pain if in fact 18% do.

35. 

### 8.2 Testing a Claim about a Proportion

1. a. 270 said yes
   b. $\hat p = .53$

3. Confidence level is not equivalent to the other two for proportions, because it uses the sample proportion, not the null hypothesis proportion.

```{r}
510 * .53
```

5. a. left tailed
   b. z (4.46) OOOPS, negative
   c. .000004115
   d. p = .1
   e. reject null hypothesis, confirm p < .1
   
7. a. two-tailed
   b. z = -1.69
   c. P value = .091
   d. p = .92, fail to reject it
   e. There is not sufficient evidence to REJECT THE CLAIM that 92% of adults own cell phones. OOPS

9. Null hypothesis is p = .10 (proportion of inaccurate orders = .10)
   Alternative hypothesis p < .10  (THEY WANTED TWO TAILED)
   z = -.56
   P value = .2875
   Fail to reject null hypothesis
   There is not sufficient evidence to claim that the proportion of inaccurate orders is < 10%
   
```{r}
n <- 362
p <- .10
phat <- 33 / n
phat
z <- (phat - p) / sqrt(p * (1-p) / n)
z
pnorm(z)

```
   
11. NO IDEA what I'm doing or interpreting wrong.  Oh, missing hyphen in assignment statement.  Fixed it.
    H null is p = .5 (p being those responding in favor)
    H alt is p <> .5
    z = 2.69
    P value = 0.007
    Reject null hypothesis
    There is sufficient evidence to reject the claim that the proportion of people who answered yes on the survey is equivalent to a coin flip.
    
```{r}
n <- 481 + 401
p <- .5
phat <- 481 / n
phat
z <- (phat - p) / sqrt(p * (1-p) / n)
z
(1 - pnorm(z)) * 2

```
    
13. H null is p = .20 proportion of users get nausea
    H alt is p>.20 
    z = 1.10
    P value = .1367
    Fail to reject null hypothesis
    There is no sufficient evidence to claim that the nausea rate is greater than 20%.
    

```{r}
n <- 227
p <- .2
phat <- 52/n
phat
z <- (phat - p) / sqrt(p * (1-p) / n)
z
1 - pnorm(z)
```

15. H null is p = .15 return rate
    H alt is p < .15
    z = -1.307
    P value = .09561
    Fail to reject null hypothesis
    There is insufficient evidence to conclude that the return rate is less than 15%.
    
    
```{r}
n <- 5000
p <- .15
phat <- 717 / n
phat
z <- (phat - p) / sqrt(p * (1-p) / n)
z
pnorm(z)
```
 
17. H null is that p = .512 
    H alt is that p <> .512
    z = -.9769
    P value = .3286
    Fail to reject null hypothesis
    There is insufficient evidence to reject the belief that 51.2% of newborns are boys.

```{r}
p <- .512
n <- 860
phat <- 426/860
phat
z <- (phat - p) / sqrt(p * (1-p) / n)
z
pnorm(z) * 2
```

31. H null is that p = .791, that the pool of Mexican jurors fairly considered was as large as the eligible Mexican population.
    H alt is that p < .791 ("biased against")
    z = -29.09
    P value = 0
    Reject the null hypothesis.
    There is sufficient evidence to conclude that the grand jury selection system is biased against choosing Mexicans.

```{r}
n <- 870
p <- .791
phat <- .39
z <- (phat - p) / sqrt(p * (1-p) / n)
z
pnorm(z)
```

33. P-value .01141
    P-value .001953 # NOPE .0215
    P-value .01172

```{r}
p <- 0.5
n <- 10
phat <- 9/10
z <- (phat - p) / sqrt(p * (1-p) / n)
z
(1 - pnorm(z)) * 2

(1 - pbinom(8, size=n, prob=p)) * 2
((1 - pbinom(9, size=n, prob=p)) + dbinom(9, size=n, prob=p) / 2) * 2
```

### 8.3 Testing a Claim About a Mean

1. The twelve games must be randomly chosen.
   There must be no outliers and the population values must be normally distributed OR the sample size must be > 30.
   The requirements are NO satisfied
   
   
```{r}
x <- c(84, 14, 583, 50, 0, 57, 207, 43, 178, 0, 2, 57)
qqnorm(x, datax=TRUE )
qqline(x, datax=TRUE )
```

3. Don't know the population standard deviation.

5. p value .0651 OOPS, times 2

```{r}
pt(-1.625, df=12) * 2
```

7. P-value .2379

```{r}
(1 - pt(1.340, df=5)) * 2
```

9. H null is mean = 4.00Mbps
   H alt is mean < 4.00Mbps
   t = -.3663
   P value = .3579
   There is not sufficient evidence to conclude that Sprint data speeds are less than 4.00 Mbps.

11. H null is mean = 0 minutes
    H alt is mean != 0 minutes
    t = -8.72
    P value = 0.0000
    There is sufficient evidence to conclude that the mean error given is not 0.

13. H null is mean = 4.0
    H alt is mean != 4.0
    t = -1.638
    P value = .1049
    There is not sufficient evidence to reject the claim that the mean course evaluation score is 4.0
    
```{r}
n <- 93
xbar <- 3.91
s <- 0.53
tstat <- (xbar - 4.0) / (s / sqrt(n))
tstat
pt(tstat, df=n-1) * 2
```
    
15. H null is mean change = 0
    H alt is mean change > 0
    t = .133
    P value = .4472
    There is not sufficient evidence to conclude that the chloresterol chnage is > 0.

```{r}
n <- 49
xbar <- 0.4
s <- 21.0
x <- 0
tstat <- (xbar - x) / (s / sqrt(n))
tstat
1 - pt(tstat, df=n-1)

```

17. H null is mean weight loss = 0
    H alt is mean weight loss > 0
    t = 3.872
    P value = .0002003
    There is sufficient evidence to conclude that people lost weight, but the amount seems practically insignificant for one year.

```{r}
n <- 40
xbar <- 3.0
s <- 4.9
x <- 0
tstat <- (xbar - x) / (s / sqrt(n))
tstat
1 - pt(tstat, df=n-1)
```

19. H null is mean = 12.0 oz.
    H alt is mean != 12.0 oz.
    t = 10.364
    P value = 0.0000

```{r}
n <- 36
xbar <- 12.19
s <- .11
x <- 12.0
tstat <- (xbar - x) / (s / n^.5)
tstat
(1 - pt(tstat, df=n-1)) * 2
```

25. Insufficient evidence to conclude that mean is less than 75.

```{r}
body <- read.table("data/Triola/01 - Body Data.txt", sep="\t", header=TRUE)
female <- body[body$GENDER..1.M. == 0,]
#female
t.test(female$PULSE, alternative="less", mu=75)

```

27.  Reject null hypothesis.  Conclude true mean is less than 90.  No, it doesn't mean NO women in sample have values >= 90.

```{r}
#female
t.test(female$DIASTOLIC, alternative="less", mu=90)
```

### 8.4 Testing a Claim about a Standard Deviation or Variance

1. Strict normality, and a random sample are required.  The normality requirement for a Chi Squared test is stricter.  It is not good enough to have a roughly symmetrical distribution with no outliers.  The requirement for normality cannot be gotten around by using a big enough sample size either.

5. H null is sd = 10 bpm
   H alt is sd != 10 bpm
   Chi Sq = 195.2
   P value = .0208
   Reject H null
   There is sufficient evidence to conclude that the standard deviation of men's heart rates is NOT 10 bpm.  
   Not true (although says nothing about how far off it might be).
   
7. H null is sd = 2.08 F
   H alt is sd < 2.08 F
   Chi Sq = 9.329
   P value = 0.0000
   Conclusion is safe in this way at least.

```{r}
n <- 106
sigma <- 2.08
s <- .62
chisq <- (n - 1) * s^2 / sigma^2
chisq
pchisq(chisq, df=n-1)
qchisq(.01, df=n-1)
```

9. H null sd = 27.8
   H alt sd != 27.8
   Chi Sq = 8.505
   P value = .03835
   Reject H null
   There is sufficient evidence to suggest that the sd for the new cans is NOT 27.8.

```{r}
n <- 20
sigma <- 27.8
s <- 18.6
chisq <- (n - 1) * s^2 / sigma^2
chisq
pchisq(chisq, df=n-1) * 2
```

11. H null sd = .15 oz
    H alt sd > .15 oz
    Chi Sq = 33.396
    P-value = .8491  NOPE, .1509
    Fail to reject H null.
    There is not sufficient evidence to support the claim that the standard deviation is higher than the advertised amount.
    
```{r}
n <- 27
sigma <- .15
s <- .17
chisq <- (n-1) * s^2 / sigma^2
chisq
1 - pchisq(chisq, df=n-1)
```

13. H null sd=32.2 feet
    H alt sd>32.2 feet
    Chi Sq = 29.176
    P-value = .0021
    Reject H null
    There is sufficient evidence to conclude that the new method makes altimiters with a wider standard deviation of errors.  Appears worse.  Find a scapegoat and keep going, obviously.
    
    
```{r}
alterr <- c(-42, 78, -22, -72, -45, 15, 17, 51, -5, -53, -9, -109)
s <- sd(alterr)
s
sigma <- 32.2
n <- length(alterr)
chisq <- (n - 1) * s^2 / sigma^2
chisq
1 - pchisq(chisq, df=n-1)
```
    
## 9. Inferences from Two Samples

### 9.1 Two Proportions

1. Yes, the number of successes and failures is both greater than 5. The children were randomly assigned.  It's fine.

3. a. Null hypothesis is that true proportions are equal
      Alt hypothesis is that true proportion of polio is less with vaccine.
   b. Reject null hypothesis

5. Evidence shows vinyl gloves do leak more.

7. $p_1$: Proportion of cars with only rear license plates
   $p_2$: Proportion of trucks with only rear license plates  
   $H_0: p_1 = p_2$
   $H_1: p_1 > p_2$

```{r}
x1 <- 239
n1 <- 2049
x2 <- 45
n2 <- 334
phat1 <- x1/n1
phat2 <- x2/n2
phat1
phat2
pbar <- (x1 + x2) / (n1 + n2)
z <- (phat1 - phat2) / sqrt((pbar*(1-pbar)/n1) + (pbar*(1-pbar)/n2))
z
pnorm(z)
1 - pnorm(z)

prop.test(x=c(x1, x2), n=c(n1, n2), alternative="greater", correct=FALSE)
prop.test(x=c(x2, x1), n=c(n2, n1), alternative="less", correct=FALSE)

e <- qnorm(.95) * sqrt((phat1*(1-phat1)/n1)+(phat2*(1-phat2)/n2))
e
(phat1 - phat2) - e
(phat1 - phat2) + e
```

9. $H_0: p_1 = p_2$ ;
   $H_1: p_1 > p_2$ ;

```{r}
n1 <- 198
x1 <- 51
n2 <- 199
x2 <- 30
prop.test(x=c(x1,x2), n=c(n1,n2), correct=FALSE, alternative="greater", conf.level=.99)
phat1 <- x1/n1
phat2 <- x2/n2
e <- qnorm(.99) * sqrt((phat1*(1-phat1)/n1)+(phat2*(1-phat2)/n2))
e
(phat1 - phat2) - e
(phat1 - phat2) + e

```

11. $H_0: p_1 = p_2$;
    $H_1: p_1 > p_2$;
    z = 6.440
    p-value = .0000
    reject $H_0$
    There is sufficient evidence to conclude that a greater proportion of people over 55 dream in black and white
    b. $.117 > p_1 - p_2 > .240$
    c. No, but it doens't disprove it either.
    

```{r}
prop.test(x=c(68, 13), n=c(306, 298), correct=FALSE, alternative="greater")
phat1 <- 68/306
phat2 <- 13/298
pbar <- (68 + 13) / (306 + 298)
z <- (phat1 - phat2) / sqrt((pbar*(1-pbar)/306)+(pbar*(1-pbar)/298))
z
1 - pnorm(z)
e <- qnorm(.99)*sqrt((phat1*(1-phat1)/306)+(phat2*(1-phat2)/298))
phat1 - phat2 - e
phat1 - phat2 + e
```

13. a. H null is that $p_1 = p_2$
       H alt is that $p_1 > p_2$
       z = 6.106
       p value = .0000
       reject H null
       There is sufficient evidence to conclude that seat belts reduce fatalities.
    b. .00495 < p1 - p2 < .01290
    

```{r}
n1 <- 2823
x1 <- 31
n2 <- 7765
x2 <- 16
(phat1 <- x1/n1)
(phat2 <- x2/n2)
(pbar <- (x1 + x2) / (n1 + n2))
(z <- (phat1 - phat2) / sqrt(pbar*(1-pbar)/n1 + pbar*(1-pbar)/n2))
1 - pnorm(z)
e <- qnorm(.95) * sqrt(phat1 * (1-phat1) / n1 + phat2 * (1-phat2) / n2)
phat1 - phat2 - e
phat1 - phat2 + e
prop.test(x = c(31, 16), n = c(n1, n2), correct=FALSE, alternative="greater", conf.level = .95)
prop.test(x = c(31, 16), n = c(n1, n2), correct=FALSE, alternative="two.sided", conf.level = .90)
```

15. 

```{r}

```

### 9.2 Two Means: Independent Samples

1. a. Not independent
   b. Not independent
   c. Independent
   
3. a. Yes, same conclusion
   b. Yes, same conclusion
   c. 98%
   
5. $H_0: \mu_1 = \mu_2$
   $H_1: \mu_1 < \mu_2$
   a. reject $H_0$, there is evidence to support diet coke cans weighing less.
   b. 

```{r}
n1 <- 36
n2 <- 36
xbar1 <- .78479
xbar2 <- .81682
s1 <- .00439
s2 <- .00751
(tstat <- (xbar1 - xbar2) / sqrt(s1^2/n1 + s2^2/n2))
pt(tstat, df=35)

e <- qt(.95, df=35) * sqrt(s1^2/n1 + s2^2/n2)
xbar1 - xbar2 - e
xbar1 - xbar2 + e
```

7. a. $H_0: \mu_\text{red} = \mu_\text{blue}$
      $H_1: \mu_\text{red} < \mu_\text{blue}$
      t = -2.9790
      p value = .0027
   b. Conf interval (-1.055, -.105)

```{r}
n1 <- 35
n2 <- 36
xbar1 <- 3.39
xbar2 <- 3.97
s1 <- .97
s2 <- .63
ssamp <- sqrt(s1^2/n1 + s2^2/n2)
(tstat <- (xbar1 - xbar2) / ssamp)
pt(tstat, df=n1 - 1)

e <- qt(.99, df=n1-1) * ssamp
xbar1 - xbar2 - e
xbar1 - xbar2 + e

```


21. p-value = .448
    Fail to reject H_0
    There is not sufficient evidence to conclude that men talk less than women.

```{r}
words <- read.table("data/Triola/24 - Word Counts.txt", header=TRUE, sep="\t")
t.test(words$M2, words$F2, alternative = "less", conf.level=.95)

```

23. $H_0: \mu_g = \mu_b$;
    $H_1: \mu_g < \mu_b$;
    t = -3.4504
    p-value = .0003098
    reject $H_0$
    There is sufficient evidence to conclude girls have lower birth weights than boys.

```{r}
births <- read.table("data/Triola/04 - Births.txt", header=TRUE, sep="\t")
colnames(births)
#t.test(BIRTH.WEIGHT ~ GENDER..1.M., data=births)
weights.girls = births[births$GENDER..1.M. == 0,]$BIRTH.WEIGHT
weights.boys = births[births$GENDER..1.M. == 1,]$BIRTH.WEIGHT
t.test(weights.girls, weights.boys, alternative = "less")
```

### 9.3 Two Dependent Samples (Matched Pairs)

1. a. Yes
   b. They are robust, which means the distribution of sample differences does NOT need to be very close to a normal distribution.
   c. Yes
   d. No, those are not matched pairs
   e. No, they require we use n=10.
   
3. Results are not affected.

5.

```{r}
oscar <- data.frame(f=c(28,28,31,29,35,26,26,41,30,34),
                    m=c(62,37,36,38,29,34,51,39,37,42))
t.test(paired=TRUE, x=oscar$f, y=oscar$m, conf.level=.95,                       alternative="less")
t.test(paired=TRUE, x=oscar$f, y=oscar$m, conf.level=.90,
       alternative="two.sided")$conf.int
```

7. $H_0: \mu_d = 0$
   $H_1: \mu_d \neq 0$
   t = -7.499
   p-value = .0001 NOPE, two tailed .0003
   reject $H_0$
   There is sufficient evidence to conclude there is a body temperature difference between 8am and noon.
   95% confidence interval -1.97F < \mu_d < -1.00F

```{r}
temps <- data.frame(morn=c(96.6, 97.0, 97.0, 97.8, 97.0, 97.4, 96.6),
                    noon=c(99.0, 98.4, 98.0, 98.6, 98.5, 98.9, 98.4))
temps$diff <- temps$morn - temps$noon
ssamp <- sd(temps$diff) / sqrt(length(temps$diff))
(tstat <- mean(temps$diff) / ssamp)
pt(tstat, df=length(temps$diff)-1) * 2
e <- qt(.975, df=length(temps$diff)-1) * ssamp
mean(temps$diff) - e
mean(temps$diff) + e

```

9. $H_0: \mu_d = 0$
   $H_0: \mu_d \neq 0$
   t = -1.379
   p-value = .2013
   Fail to reject $H_0$
   There is not sufficient evidence to conclude that there is a height difference between mothers and daughters.

```{r}
h <- data.frame(m=c(68,60,61,63.5,69,64,69,64,63.5,66),
                d=c(68.5,60,63.5,67.5,68,65.5,69,68,64.5,63))
h$diff <- h$m - h$d
ssamp <- sd(h$diff) / sqrt(length(h$diff))
(tstat <- mean(h$diff) / ssamp)
pt(tstat, df=length(h$diff)-1) * 2
t.test(x=h$m, y=h$d, conf.level=.95, alternative="two.sided",
       paired=TRUE)
```

## 10. Correlation and Regression

### 10.1 Correlation

1. a. r represents the correlation in the sample.
      $\rho$ represents the correlation in the population of statistics students.
   b. 0
   c. No

3. No.  It doesn't rule it out, but there could be lurking variables.  NOT SURE IF THIS WAS RIGHT OR NOT.

5. Yes, yes, yes

7. r critical
value about .254.  No there is insufficient evidence to conclude there is a linear correlation

9. .8162
   The high, arching shape showing near perfect alignment to a curve, rather than a line.

```{r}
df <- data.frame(x=c(10,8,13,9,11,14,6,4,12,7,5),
                 y=c(9.14,8.14,8.74,8.77,9.26,8.10,6.13,3.10,9.13,7.26,4.74))
plot(df)
res <- lm(df)
summary(res)
sqrt(.6662)
sqrt(.6292)
cor(df)
```

11. a. Maybe, but only if the outlier is real.
    b. .9057, yes
    c. No, definately not and
       0, no
    d. Wow, check for outliers before determining correlation!

```{r}
x <- c(1,1,1,2,2,2,3,3,3)
y <- c(1,2,3,1,2,3,1,2,3)
plot(x,y)
cor(x,y)
xplus <- c(x,10)
yplus <- c(y,10)
plot(xplus,yplus)
cor(xplus,yplus)
df <- data.frame(x=xplus, y=yplus)
summary(lm(df))
sqrt(.8202)

sum((xplus - mean(xplus)) * (yplus - mean(yplus))) / ((length(xplus) - 1)  * sd(xplus) * sd(yplus))
cor(xplus,yplus)
```

13. r = .7991
    crit r is .811 by table
    p-value = .05648 by R lm()
    No, there is not sufficient evidence to conclude there is a correlation between internet users and nobel laureates.

```{r}
il <- data.frame(i=c(79.5,79.6,56.8,67.6,77.9,38.3),
                 n=c(5.5,9.0,3.3,1.7,10.8,0.1))
plot(il)
cor(il)
summary(lm(il))
```

17. r = .591 (r crit .878)
    p-value = .294
    No, insufficient evidence.

```{r}
shoe=c(29.7, 29.7, 31.4, 31.8, 27.6)
h=c(175.3, 177.8, 185.4, 175.3, 172.7)
df <- data.frame(shoe=shoe, h=h)
plot(df)
cor(df)
summary(lm(df))

```

19. r = -.959, p-value=.0099. Yes.  No.

```{r}
lem <- c(230, 265, 358, 480, 530)
fat <- c(15.9, 15.7, 15.4, 15.3, 14.9)
cor(lem, fat)
summary(lm(fat~lem, data=data.frame(lem=lem, fat=fat)))
sqrt(.9196)
```


35.

```{r}
x <- c(2,3,20,50,95)
y <- c(.3,.5,1.3,1.7,2.0)
cor(x,y)
cor(y,x)
cor(y,x^2)
cor(y,log(x))
cor(y,log10(x))
cor(y,sqrt(x))
cor(y,1/x)
plot(x,y)
plot(y,x)
plot(y,x^2)
plot(y,log(x))
plot(y,log10(x))
plot(y,sqrt(x))
plot(y,1/x)

```

### 10.2 Regression

1. a. $\hat y = -368 + 130x$
   b. It represents the estimate of the y value based solely on the x value.
   
3. a. A residual is the difference between the actual value of y and the value of $\hat y$ predicted from the value of x and the regression equation.
   b. It best fits in the sense that it minimizes the SUM OF square of the residuals.
   
5. 5.9, because the p-value says it's not significant

```{r}
8.18 - .345 * 8
```

7. 92

```{r}
-106 + 1.1*180
```

9. $\hat y = -3.001 + .5 x$
   There is a non-linear curve not reflected in the equation.

```{r}
x <- c(10,8,13,9,11,14,6,4,12,7,5)
y <- c(9.14,8.14,8.74,8.77,9.26,8.10,6.13,3.10,9.13,7.26,4.74)
df <- data.frame(x=x, y=y)
plot(df)
res <- lm(y~x, data=df)
summary(res)
abline(res)
```

11. a. $\hat y = .264 + .906x$
    b. $\hat y = 2 + 0x$
    c. Part a has a slope.  Part b, nope.  (BECAUSE ONE OUTLIER CAN DRAMATICALLY AFFECT THE REGRESSION EQUATION).

```{r}
x <- c(1,1,1,2,2,2,3,3,3)
y <- c(1,2,3,1,2,3,1,2,3)
plot(x,y)
cor(x,y)
res <- lm(y~x)
res
abline(res)

xplus <- c(x,10)
yplus <- c(y,10)
plot(xplus,yplus)
cor(xplus,yplus)
res <- lm(yplus~xplus)
res
abline(res)
```

13. $\hat y = -8.44 + .203 x$
    5.067
    Not very good.


```{r}
il <- data.frame(n=c(5.5,9.0,3.3,1.7,10.8,0.1),
                 i=c(79.5,79.6,56.8,67.6,77.9,38.3))
plot(il)
cor(il)
summary(lm(il))
mean(il$n)

```

15. $\hat y = -.011 + 1.011 x $
    $3.02
    Not exactly, because 2 cents is an odd number.  It is likely to be close, however.
    

```{r}
pizza <- c(.15, .35, 1.0, 1.25, 1.75, 2.0, 2.25, 2.30, 2.75)
fare <- c(.15, .35,1.0, 1.35, 1.50, 2.0, 2.25, 2.50, 2.75)
plot(pizza, fare)
res <- lm(fare ~ pizza)
abline(res)
summary(res)
res$coefficients['(Intercept)'] + res$coefficients['pizza'] * 3
```

17. $\hat y = 125.407 + 1.727 x$
    177.3
    NO

```{r}
shoe=c(29.7, 29.7, 31.4, 31.8, 27.6)
h=c(175.3, 177.8, 185.4, 175.3, 172.7)
plot(shoe, h)
res <- lm(h ~ shoe)
summary(res)
mean(h)

```

19. $\hat y = 16.491 - .00282 x$
    15.1
    No?  
     

```{r}
lem <- c(230, 265, 358, 480, 530)
fat <- c(15.9, 15.7, 15.4, 15.3, 14.9)
res <- lm(fat ~ lem)
summary(res)
16.4909102 - 0.0028205 * 500
```

21. $\hat y = 51.619 - .165 x$
    42.709 years (45 it said, OOHH, it's the mean because regression not predictive.)
    No.

```{r}
f <- c(28,30,29,61,32,33,45,29,62,22,44,54)
m <- c(43,37,38,45,50,48,60,50,39,55,44,33)
plot(f, m)
cor(f, m)
res <- lm(m ~ f)
abline(res)
summary(res)
res$coefficients["(Intercept)"] + res$coefficients["f"]*54
51.619 - .165 * 54
mean(m)
```

23. $\hat y = -156.879 + 40.182 x$
    -76.515,
    nope

```{r}
width <- c(7.2, 7.4, 9.8, 9.4, 8.8, 8.4)
weight <- c(116, 154, 245, 202, 200, 191)
plot(width, weight)
cor(width, weight)
res <- lm(weight ~ width)
abline(res)
summary(res)
res$coefficients["(Intercept)"] + res$coefficients["width"] * 2
```

25. $14.51
    15%

```{r}
bill <- c(33.46, 50.68, 87.92, 98.84, 63.60, 107.34)
tip <- c(5.50, 5.00, 8.08, 17.00, 12.00, 16.00)
plot(bill, tip)
res <- lm(tip ~ bill)
summary(res)

(b1 <- cor(bill, tip) * sd(tip) / sd(bill))
(b0 <- mean(tip) - b1 * mean(bill))

b0 + b1 * 100
```

### 10.3 Rank Correlation

1. Not necessarily.  The methods of section 10-2 depend on normally distributed data.  NOPE, the difference is that rank correlation can sometimes detect NON-LINEAR correlations.

3. 

$r$ is the linear correlation of a sample.
   
$r_s$ is the Spearman's rank correlation of a sample.

$\rho$ is the linear correlation of a population.

$\rho_s$ is the Spearman's rank correlation of a population

No, the s subscript represents the last name of Spearman, the person who invented this technique.

5. 1

6. -1

7. 

```{r}
choc <- c(11.6, 2.5, 8.8, 3.7, 1.8, 4.5, 9.4, 3.6, 2.0, 3.6, 6.4)
nobel <- c(12.7, 1.9, 12.7, 3.3, 1.5, 11.4, 25.5, 3.1, 1.9, 1.7, 31.9)
cor.test(choc, nobel, method="spearman")
```

9. $r_s=1$
   Reject the null hypothesis of no correlation
   

```{r}
pizza <- c(.15, .35, 1.0, 1.25, 1.75, 2.0, 2.25, 2.30, 2.75)
fare <- c(.15, .35,1.0, 1.35, 1.50, 2.0, 2.25, 2.50, 2.75)
cor.test(pizza, fare, method="spearman")
```

13. $r_s = .9023$  Reject null.  There is evidence of a correlation.

```{r}
nobel <- read.table("data/Triola/16 - Nobel Laureates and Chocolate.txt", header=TRUE, sep="\t")
cor.test(x=nobel$CHOCOLATE, y=nobel$NOBEL, method="spearman")
```

## 11. Chi-Square and Analysis of Variance

### 11.1 Goodness-of-Fit

1. a. Observed $O$,
      Expected $E$
   b. $O_2 = 62$,
      $E_2 = 55.792$
   c. .691
   
```{r}
arr <- c(76,62,29,33,19,27,28,21,22)
(ex <- sum(arr) * .176)
(arr[2] - ex)^2 / ex
```
   
3. Reject null hypothesis that it fits.

5. Fail to reject

7. $\chi ^2 = 5.86$
   crit 11.071
   fail to reject null hypothesis
```{r}
loaded <- c(27,31,42,40,28,32)
(ex <- sum(loaded) / length(loaded))
sum((loaded - ex)^2 / ex)
```

9. $\chi^2 = 11.161$
   critical value 7.815
   reject null hypothesis
   There is sufficient evidence to conclude that the results DO contradict Mendel's theory.

```{r}
ob <- c(307,77,98,18)
ratio <- c(9,3,3,1)
prob <- ratio / sum(ratio)
(ex <- prob * sum(ob))
sum((ob - ex)^2 / ex)
```

11. $\Chi^2 = 29.814
    crit = 16.812
    Reject null hypothesis.  there is sufficient evidence to conclude that the days of the week do not have the same number of police calls.  More calls toward the weekend.

```{r}
ob <- c(114,152,160,164,179,196,130)
ex <- sum(ob) / length(ob)
sum((ob - ex)^2 / ex)
```

### 11.2 Contingency Tables

1. a. 4.173
   b. not met for $\chi^2$ approximation 

```{r}
43 * (16 + 50 + 3) / (436 + 166 + 40 + 16 + 50 + 3)
```

3. $\chi^2 = 64.517$
   p-value = .000
   Reject null hypothesis.  Evidence that proportions are not the same. EVIDENCE THAT THE TWO AXES ARE NOT INDEPENDENT.
   
5. $\chi^2 = 25.571
   crit (for 1 df) 3.841
   Reject H null, There is evidence that a polygraph result is NOT independent of whether someone lied.
   

```{r}
ob <- matrix(c(15,32,42,9), nrow=2)
ob
ex <- matrix(c(
    sum(ob[1,]) * sum(ob[,1]) / sum(ob),
    sum(ob[2,]) * sum(ob[,1]) / sum(ob),
    sum(ob[1,]) * sum(ob[,2]) / sum(ob),
    sum(ob[2,]) * sum(ob[,2]) / sum(ob)
), nrow=2)
ex
sum((ob - ex)^2 / ex)
```

7. $\chi^2 = 576.224$
   crit value = 3.841
   Reject null hypothesis.  The risky behaviors are not independent.

```{r}
ob <- matrix(c(731, 156, 3054, 4564), nrow=2)
ob
ex <- matrix(c(
    sum(ob[1,]) * sum(ob[,1]) / sum(ob),
    sum(ob[2,]) * sum(ob[,1]) / sum(ob),
    sum(ob[1,]) * sum(ob[,2]) / sum(ob),
    sum(ob[2,]) * sum(ob[,2]) / sum(ob)
), nrow=2)
ex
sum((ob - ex)^2 / ex)
```

9. $\chi^2 = 12.162$
   crit value = 3.841
   Reject null hypothesis.  They are not independent.  There is evidence for a denomination effect.

```{r}
ob <- matrix(c(27, 12, 16,34), nrow=2)
ob
ex <- matrix(c(
    sum(ob[1,]) * sum(ob[,1]) / sum(ob),
    sum(ob[2,]) * sum(ob[,1]) / sum(ob),
    sum(ob[1,]) * sum(ob[,2]) / sum(ob),
    sum(ob[2,]) * sum(ob[,2]) / sum(ob)
), nrow=2)
ex
sum((ob - ex)^2 / ex)
```

11. $\chi^2 = .0637$
    crit value = 3.841
    Fail to reject null hypothesis, no evidence of call success being dependent on gender.

```{r}
ob <- matrix(c(161, 68, 376, 152), nrow=2)
ob
ex <- matrix(c(
    sum(ob[1,]) * sum(ob[,1]) / sum(ob),
    sum(ob[2,]) * sum(ob[,1]) / sum(ob),
    sum(ob[1,]) * sum(ob[,2]) / sum(ob),
    sum(ob[2,]) * sum(ob[,2]) / sum(ob)
), nrow=2)
ex
sum((ob - ex)^2 / ex)
```

### 11.3 One-Way Analysis of Variance

1. a. We use a one-way ANOVA because the data are only divided up by one category, the flight number.

   b. The technique is called analysis of variance because it analyzes the ratio of the variance between the samples to the variance within the samples.
   
3. F is 1.334, Uses F distribution with two different degrees of freedom.

5. We cannot reject the null hypothesis that the three lead groups have the same mean IQ.  This provides no evidence that lead affects verbal IQ scores.

7. F=5.596, p-value = .00455, there is significant evidence that the mean drive through time are NOT the same.

9. F=7.93, p-value = .000539, there is significant evidence that the mean pulse rates are NOT the same.

13.  OHHH, LOOKS LIKE I DID DO IT RIGHT, BUT EQUAL VARIANCES REQUIREMENT NOT MET.

```{r}
df <- data.frame(arr=c(-2, -1, -2, 2, -2, 0, -2, -3, 
                       19, -4, -5, -1, -4, 73, 0, 1,
                       18, 60, 142, -1, -11, -1, 47, 13),
                 flight=c(rep("f1", 8), rep("f19", 8), rep("f21", 8)))
res <- aov(df$arr ~ df$flight)
summary(res)
TukeyHSD(res)
boxplot(df$arr ~ df$flight)
```

15.

```{r}
chips <- read.table("data/Triola/28 - Chocolate Chip Cookies.txt", header=TRUE, sep="\t")
chips
```

