---
title: "Cheatsheet"
output: html_document
---

## Terms

**Population** - The entire group of things that we ultimately want to learn about.

**Sample** - A subset of the population which we study to learn about the *population*.

**Statistic** - A property of the sample.  Used to estimate a *parameter* of the *population*.

**Parameter** - A property of the population.  Estimated by a *statistic* of the *sample*.

**Qualitative** - Categorical.  Example brown, blue, green, gray.

**Quantitative** - Numerical.  Example, a count of people or a measurement of an amount of water.

**Discrete** - Non-continuous *quantitative* data.  Things that can be counted.  Example, you can have 4 or 5 people, but not 4 1/2 people.

**Continuous** - *Quantitative* data that can be measured continuously.  Example, amount of water, you can have 4 gallons, 5 gallons, 4.5 gallons, 4.0001 gallons, etc.

### Levels of Measurement

**Nominal scale** - Unordered *categorical* data.  Example, favorite foods.  Sushi is not inherently less than or greater than pizza.  Think "name".

**Ordinal scale** - Ordered *categorical* data.  You can put them in order, but the difference between values cannot be measured.  Example, "strongly agree", "agree", "disagree", "strongly disagree".  Think "order".

**Interval scale** - Ordered *quantitative* data where differences can be measured, but ratios cannot be taken because there is no true 0 point.  Example, Fahrenheit scale, where the difference between 60 and 80 is 20, but where 80 is not four times as hot as 20.  Think "subtraction".

**Ratio scale** - Ordered *quantitative* data where differences and ratios can both be calculated.  Example, Kelvin scale or test scores counted from 0.  Think "multiplication".

### Random Sampling
"A sample should have the same characteristics as the population it is representing." - [OpenSTAX Introductory Statistics](https://openstax.org/details/books/introductory-statistics) book, by Barbara Illowsky and Susan Dean.

**Simple Random Sample** - Each sample of the same size is equally likely.

**Stratified Sample** - Divide population into groups called strata, and take a proportionate number from each stratum.

**Cluster Sample** - Divide population into groups called clusters, randomly select some clusters, and take all members from the selected clusters.

**Systematic Sample** - Randomly select a starting point, then take every n<sup>th</sup> item from a listing, wrapping around to the start.  Example, 20,000 names in a phone book, to get a sample of 400, choose a starting point and pick every 50<sup>th</sup> name.

**Convenience Sample** - *NOT random!  May be highly biased!*  Just use sample that is easily gotten.

Sampling can be done with or without replacement.  True random sampling is done **with replacement**, where the same member of a population can be chosen more than once.  However, with a large population and small sample, sampling **without replacement** is approximately the same, and more frequently done.

## Probability

$$P(E|F)\text{ Probability of E given F}$$
$$P(E~and~F) = P(E|F)P(F)$$
$$P(E|F) = \frac{P(E~and~F)}{P(F)}$$
$$P(E) = P(E|F)\text{ iff E and F are independent}$$
Bayes Theorem
$$P(E|F) = \frac{P(E)P(F|E)}{P(E)P(F|E) + P(not~E)P(F|not~E)} = \frac{P(E)P(F|E)}{P(F)}$$

## Descriptive Statistics

Note that relative frequency $\frac{n_x}{n}$ approaches probability $p(x)$ as sample size increases.

| | Sample | Population (Model) | |
| --- | --- | --- | --- |
| | **statistic** - Information about a sample (e.g. sample mean). | **parameter** - Information about the population (e.g. population mean). |
| | | **Discrete** | **Continuous** |
| Mean | $$\bar{x} = \sum_{i=1}^n \frac{x_i}{n}$$ | $$\mu = \sum_{all~x} x~p(x)$$ | $$\mu = \int_{-\infty}^\infty x ~ f(x)dx$$ |
| Variance | $$s^2 = \sum_{i=1}^n\frac{(x_i-\bar{x})^2}{n - 1}$$ | $$\sigma^2 = \sum_{i=1}^n\frac{(x_i-\mu)^2}{n} \\ \sigma^2 = \sum_{all~x}(x-\mu)^2~p(x) $$ | $$\sigma^2 = \int_{-\infty}^\infty(x-\mu)^2~f(x)dx $$ |
| Standard Deviation | $$s = \sqrt{s^2} $$ | $$\sigma = \sqrt{\sigma^2} $$ | $$\sigma = \sqrt{\sigma^2} $$ |
| Covariance| $$\text{cov}(x,y) = \frac{1}{n-1} \sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})$$ | | |
| Correlation | $$ r(x,y) = \frac{1}{n-1} \sum_{i=1}^n\left(\frac{x_i-\bar{x}}{s_x}\right)\left(\frac{y_i-\bar{y}}{s_y}\right) = \frac{\text{cov}(x,y)}{s_xs_y} $$ | | |
| Least squares linear regression| $$ \begin{align*} \hat y_i &= \hat\alpha + \hat\beta x_i\\ \hat\beta &= \frac{\text{cov}(x,y)}{\text{var}(x)} = \frac{rs_y}{s_x} \\ \hat\alpha &= \bar y - \hat\beta \bar x \end{align*} $$| $$ y_i = \alpha + \beta x_i + \epsilon_i $$ | |

## Significance, Errors and Power

Findings \\ True State | $H_0$ is true | $H_0$ is false |
| --- | --- | --- |
| Reject $H_0$ |  Reject true $H_0$, P(Type I error) = $\alpha$ | Correct |
| Fail to reject $H_0$* | Correct | Fail to reject false $H_0$, P(Type II error) = $\beta$ (for a selected alternate value) |

*The **null hypothesis** ($H_0$) cannot be proven or accepted.  We can only reject it, or fail to reject it.  [I think this is mainly because it is the hypothesis associated with an equality.]

*Here is an attempt at coloring under part of a normal curve in R, for use in whatever context, but possibly for illustrating power*

```{r}
left <- qnorm(.025)
right <- qnorm(.975)
plot(dnorm, xlim=c(-3,6), xaxt="n")
axis(1, c(seq(-3,6,1)))

# Add colored in area under standard normal curve
x <- seq(left, right, .01)
y <- dnorm(x)
x <- c(left, x, right)
y <- c(0, y, 0)
abline(h=0)
polygon(x, y, col="papayawhip")

# Draw alternate normal curve with mean of alt.
alt <- 3
xalt <- seq(-3, 6, .01)
yalt <- dnorm(xalt, mean=alt, sd=1)
lines(xalt, yalt)

# Draw possible power area under alternate curve
xalt <- seq(right, 6, .01)
yalt <- dnorm(xalt, mean=alt, sd=1)
xalt <- c(right, xalt, 6)
yalt <- c(0, yalt, 0)
polygon(xalt, yalt, col="mediumaquamarine")

```


## Distributions

Some of these are defined using a gamma function.  Note that $\Gamma(t+1) = t \Gamma(t)$ and, for non-negative integers, $\Gamma(n) = (n-1)!$
$$
\Gamma(s) = \int_0^\infty x^s e^{-x} \frac{dx}{x}
$$

The $\nu$ used in some of these is also known as the **degrees of freedom**.

See also [An Introduction to the Science of Statistics: From Theory to Implementation, Preliminary Edition](https://www.math.arizona.edu/~jwatkins/statbook.pdf) Chapter 9

| Name | State Space | Mass/Density Formula | R family | Mean | Variance | Standard Deviation |
| --- | --- | --- | --- | --- | --- | --- |
| Bernoulli | $S = \{0,1\}$ | $$ 
\begin{align*}
f_X(x|p) &= 
\begin{cases}
0 & \text{with probability } (1-p) \\
1 & \text{with probability } p
\end{cases}
\\
&= p^x(1-p)^{1-x}
\end{align*}
$$ | `binom(size=1, ...` | $\mu = p$ | $\sigma = p(1-p)$ | |
| Binomial | $S = \{0, 1, \dots , n\}$ | $$
f_X(x|p) = { n \choose x } p^x(1-p)^{n-x}
$$ | `binom` | $\mu = np$ | $\sigma ^2=np(1-p)$ | $\sigma=\sqrt{np(1-p)}$ |
| Geometric | $S=\mathbb{N}$ | $$
f_X(x|p) = p(1-p)^x
$$ | `geom` | $\mu=\frac{1-p}{p}$ | $\sigma^2=\frac{1-p}{p^2}$ | |
| Poisson | $S=\mathbb{N}$ | $$
f_X(x|\lambda) = \frac{\lambda^x}{x!} e^{-\lambda}
$$ | `pois` | $\mu=\lambda$ | $\sigma^2=\lambda$ | $\sigma=\sqrt{\lambda}$ |
| Uniform (discrete) | $S= \{a, a+1, \dots, b\}$ | $$
f_X(x|a,b) = \frac1{b-a+1}
$$ | `sample(..., replace=TRUE)` | $\mu=\frac{b-a+1}{2}$ | $\sigma^2=\frac{(b-a+1)^2-1}{12}$ |
| Uniform (continuous) | $S=[a,b]$ | $$
f_X(x|a,b) = \frac{1}{b-a}
$$ | `unif` | $\mu=\frac{a+b}{2}$ | $\sigma^2=\frac{(b-a)^2}{12}$ |
| Normal | $S=\mathbb{R}$ | $$
f_X(x|\mu,\sigma) = \frac{1}{\sigma\sqrt{2\pi}} exp \left(-\frac{(x-\mu)^2}{2\sigma^2} \right)
$$ | `norm` | $\mu=\mu$ | $\sigma^2=\sigma^2$ |
| Exponential | $S=[0,\infty)$ | $$
f_X(x|\lambda) = \lambda e^{-\lambda x}
$$ | `exp` | $\mu=\frac{1}{\lambda}$ | $\sigma=\frac{1}{\lambda^2}$ |
| Gamma | $S=[0,\infty)$ | $$
f(x|\alpha,\beta) = \frac{\beta^\alpha}{\Gamma(\alpha)} x^{\alpha-1} e^{-\beta x} $$ | `gamma` | $\mu = \frac{\alpha}{\beta}$ | $\sigma^2=\frac{\alpha}{\beta^2}$ |
| Chi-Square | $S=[0,\infty)$ | $$
f_X(x|\nu) = \frac{x^{\nu/2-1}}{2^{\nu/2} \Gamma(\nu/2)} e^{-x/2}
$$ | `chisq` | $\mu=\nu$ | $\sigma^2=2\nu$ |
| Students t | $S=\mathbb{R}$ | $$
f_X(x|\nu,\mu,\sigma) = \frac{\Gamma((\nu+1)/2)}{\sqrt{\nu\pi}\,\Gamma(\nu/2)\sigma} \left(1 + \frac{(x - \mu)^2}{\nu\sigma^2} \right) ^{-(\nu+1)/2}
$$ | `t` | $\mu=a, (\nu>1)$ | $\sigma^2 = \sigma^2 \frac{a}{a-2}, (\nu > 2)$ |

## Credits

* [OpenSTAX Introductory Statistics](https://openstax.org/details/books/introductory-statistics) book, by Barbara Illowsky and Susan Dean.
* [An Introduction to the Science of Statistics: From Theory to Implementation, Preliminary Edition](https://www.math.arizona.edu/~jwatkins/statbook.pdf) by Joseph C. Watkins
* Essentials of Statistics, 6th Ed., by Mario F. Triola
