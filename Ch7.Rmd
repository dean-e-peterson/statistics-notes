---
title: "Chapter 7 - Random Variables and Distribution Functions"
output: html_document
---

*Notes from working through [An Introduction to the Science of Statistics: From Theory to Implementation, Preliminary Edition](https://www.math.arizona.edu/~jwatkins/statbook.pdf) by Joseph C. Watkins*

```{r setup, include=FALSE}
knitr::opts_chunk$set(comment = NA)
```

## 7.1 Introduction

| Statistics | Probability |
| --- | --- |
| Empirical cumulative distribution function | Cumulative distribution function |
| Sample means and variances | Distributional means and variances |

**Random Variable** - A function that takes in something from the probability space and spits out a real number.
$$ X:\Omega \rightarrow \mathbb{R} $$

## 7.2 Distribution Functions

A **cumulative distribution function** of a random variable X is defined by
$$ F_X(x) = P\{\omega \in \Omega ; X(\omega) \leq x\} $$
Or, in more abbreviated notation...
$$ F_X(x) = P\{X \leq x\} $$
*Exercise 7.9* Let $X$ be the number of heads on three independent flips of a *biased* coin that gives heads with probability $p$.  Give the cumulative distribution function $F_X$ for $X$.

*Exercise 7.10* Let $X$ be the number of spades in a collection of three cards.  Give the cumulative distribution function for $X$ and plot it with R.

| Spades | 0 | 1 | 2 | 3 |
| --- | --- | --- | --- | --- |


```{r}
spades <- 0:3
f <- ( (choose(13, spades) * choose(39, 3 - spades)) / choose(52,3) )
f
Fx <- cumsum(f)
Fx
plot(spades, Fx, type="s", ylim=c(0,1))
```

*Exercise 7.11* Find the cumulative distribution function of $Y=X^3$ in terms of $F_X$, the distribution function for $X$.

## 7.3 Properties of Distribution Function

A distribution function $F_X$ starts at 0, ends at 1, and does not decrease as $x$ increases.

*Exercise 7.17* [See book] $\pi \left(\frac23\right)^2 - \pi \left(\frac13\right)^2 = \frac13 \pi$

$$
\begin{align*}
A &= \pi r^2 & A&=\pi 1^2 & A=\pi   \\
r &= \sqrt\frac{A}{\pi} \\
\text{3rd quartile, } r_{third} &= \sqrt \frac34 \\
\text{median, } r_{median} &= \sqrt \frac12 \\
\text{1st quartile, } r_{first} &= \sqrt \frac14 \\
\end{align*}
$$

I believe distribution functions in R are prefixed with `p` for probability (such as `pnorm()`).



## 7.4 Mass Functions

The **(probability) mass function** of a discrete random variable $X$ is
$$ f_X(x) = P\{X = x\} $$

It is always positive, and sums to 1 for all $x$.


## 7.5 Density Functions

The derivative of the (cumulative) distribution function $F_X$ is the **probability density function** $f_X$

$$
\begin{align*}
F_X &= \int_{-\infty}^x f_X(t)\,dt \\
f_X &= \lim_{\Delta x \rightarrow 0} \frac{F_X(x+\Delta x) - F_X(x)}{\Delta x} = F'_X(x) \\ \\
F_X(x+\Delta x) - F_X(x) & \approx f_X(x)\Delta x \\ \\
P\{a<X\leq b\} &= F_X(a) - F_X(b) = \int_a^b f_X(t)dt
\end{align*}
$$

*Exercise 7.28* For any number $x_0$, $P\{X = x_0\}$ because the width under the curve at any point is 0, giving $\Delta x = 0$, and therefore $f_X(x)\Delta x = 0$ regardless of what $f_X$ is ?

*Exercise 7.29* Plot, on both the distribution function and density function, the probability that the dart from [one of the problems above] lands between 1/3 and 2/3 unit from the center.

```{r}
x <- seq(1/3, 2/3, .01)
# x <- c(.33, x, .67)
FX <- pi * x^2
fx <- 2 * x
plot(x, FX, xlim=c(0,1), ylim=c(0,FX[length(FX)]), col="blue", type="l")
lines(x, fx)
  
```

R can differentiate with the `D()` command!  Here is an example for the exponential distribution.

```{r}
F <- expression(1-exp(-lambda*x))
f <- D(F, "x")
f
```


## Dean's Distribution Interlude

Here are some distributions graphed in R, with the mass or density function in black, and the cumulative distribution function in blue.

The normal distribution.
```{r}
curve(pnorm, xlim=c(-3,3), col="blue") # Normal cumulative probability.
curve(dnorm, add=T)                    # Normal density
abline(h=.5, v=0, col="lightgray")
```

Test with a bunch of generated normally-distributed random data.

```{r}
normies <- rnorm(100000)
# plot(cumsum(sort(normies)) / 100000)
hist(normies, freq=TRUE, breaks=seq(-5,5,1))
hist(normies, freq=TRUE, breaks=seq(-5,5,.1))
hist(normies, freq=FALSE, breaks=seq(-5,5,.1))
```

The binomial distribution (for 50 trials, 50% probability)
```{r}
plot(pbinom(q=0:50, size=50, prob=0.5), type="s", col="blue")
points(dbinom(x=0:50, size=50, prob=0.5))
```

The exponential distribution
```{r}
plot(pexp(seq(0,10,.1)), col="blue", pch=".", cex=2)
points(dexp(seq(0,10,.1)), pch=".", cex=2)
```

The geometric distributon (failures in a sequence of Bernoulli trials before success)

```{r}
plot(pgeom(q=0:50, prob=.1), col="blue", ylim=c(0,1))
points(dgeom(x=0:50, prob=.1))
```

## 7.6 Mixtures

Such as 0 and 1 and $\pi$ and $1 - \pi$, or a bunch of $\pi_i$ summing to 1.  You can consider the $\pi_i$ as weights and just add up both the cumulative distribution function and the density functions, if I 
understood the book correctly.

*Exercise 7.36* Find the mixture of the three mass functions with weights 1/4, 1/4, 1/2.

```{r}
f1 <- c(.2, .3, .1, .4,  0)
f2 <- c(.5, .5,  0,  0,  0)
f3 <- c(.1, .1, .2, .2, .4)
df <- data.frame(f1, f2, f3)
df$mix <- .25 * f1 + .25 * f2 + .5 * f3
df
```

## 7.7 Joint and Conditional Distributions

[Say something about this?  It basically seems to work out as you'd expect.]

Multiple random variables

### 7.7.1 Discrete Random Variables

**Joint probability mass function**

$$ f_{X_1,X_2}(x_1, x_2) = P\{X_1 = x_1, X_2 = x_2\} $$

For **independent** discrete random variables *only*, we can additionally say (from section 7.7.3)

$$  f_{X_1,X_2}(x_1, x_2) = P\{X_1 = x_1, X_2 = x_2\} = P\{X_1 = x_1\}P\{X_2 = x_2\} = f_{X_1}(x_1)f_{X_2}(x_2) $$

**Marginal probability mass function** - Add up the rows or columns.  That's probably where the name came from.

$$ f_{X_1}(x_1) = P \{X_1 = x_1\} = \sum_{x_2} P\{X_1 = x_1, X_2 = x_2\} = \sum_{x_2} f_{X_1,X_2}(x_1, x_2) $$

*Exercise 7.38*
```{r}
m <- matrix(c(.09, .07, .1, .01, .04, 0, .06, .08, .03, .07, .05, .09, .01, .02, .08, .05, .02, .03, .06, .04), nrow=4)
m
rowSums(m)
colSums(m)
sum(rowSums(m))
sum(colSums(m))
```

**Conditional mass functions** is the probabilities of a random variable taking on a value given a value for a second random variable.

$$ f_{X_2|X_1}(x_2|x_1) = P\{X_2 = x_2|X_1 = x_1\} = \frac{P\{X_1 = x_1, X_2 = x_2\}}{P\{X_1 = x_1\}} = \frac{f_{X_1, X_2}(x_1, x_2)}{f_{X_1}(x_1)} $$

*Exercise 7.40*

```{r}
# Since R counts down the row first for elements of a matrix, transpose...
t(t(m) / rowSums(t(m)))
```

### 7.7.2 Continuous Random Variables

The **joint probability density function**
$$ P\{x_1 < X_1 \leq x_1 + \Delta x_1, x_2 < X_2 \leq x_2 + \Delta x_2\} \approx f_{X_1, X_2}(x_1, x_2) \Delta x_1 \Delta x_2 $$

For **independent** continuous random variables *only*, we can additionally say.

$$ f_{X_1,X_2} (x_1,x_2) = f_{X_1}(x_1) F_{X_2}(x_2) $$

**Marginal probability density function**

$$ f_{X_1}(x_1) = \int_{-\infty}^\infty f_{X_1,X_2}(x_1,x_2)\, dx_2$$

**Conditional density function**

$$ f_{X_2|X_1}(x_2|x_1) = \frac{f_{X_1, X_2}(x_1, x_2)}{f_{X_1}(x_1)} $$

### 7.7.3 Independent Random Variables

See formulas marked with **independent** above.
 
[See also comments in book on parameterized functions and **likelyhood**.]

## 7.8 Simulating Random Variables

